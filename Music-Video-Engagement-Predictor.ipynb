{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd_plotting\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas.plotting as pd_plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = 'Spotify_Youtube.csv'\n",
    "TARGET_COLUMN = 'Views' # Target for regression task\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "music_df = pd.read_csv('Spotify_Youtube.csv')\n",
    "print(music_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- 3: Loading Data from {FILEPATH} ---\")\n",
    "df = pd.read_csv(FILEPATH)\n",
    "print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "print(\"\\n--- Initial Data Info ---\")\n",
    "print(df.info())\n",
    "print(\"\\n--- Data Head ---\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 4: Cleaning Data ---\")\n",
    "\n",
    "# --- 4a: Drop Unnamed Column ---\n",
    "if df.columns[0].startswith('Unnamed:'):\n",
    "    print(f\"Dropping the first unnamed column: {df.columns[0]}\")\n",
    "    df = df.iloc[:, 1:]\n",
    "else:\n",
    "    print(\"First column is not unnamed, not dropping.\")\n",
    "\n",
    "# --- 4b: Handle Missing Values ---\n",
    "initial_rows = len(df)\n",
    "df.dropna(inplace=True)\n",
    "rows_dropped = initial_rows - len(df)\n",
    "if rows_dropped > 0:\n",
    "    print(f\"Dropped {rows_dropped} rows with missing values.\")\n",
    "else:\n",
    "    print(\"No rows with missing values found.\")\n",
    "print(f\"DataFrame shape after cleaning: {df.shape}\")\n",
    "\n",
    "# --- 4c: Placeholder for Outlier Handling ---\n",
    "print(\"\\nPlaceholder: Outlier detection and handling needed.\")\n",
    "\n",
    "# --- 4d: Placeholder for Incorrectly Labeled Points ---\n",
    "print(\"Placeholder: Handling of incorrectly labeled points needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 5: Encoding Categorical Features ---\")\n",
    "label_encoders = {}\n",
    "# Identify object columns that are likely categorical\n",
    "# Adjust this list based on actual dataset analysis\n",
    "cols_to_encode = ['Track', 'Artist', 'Album'] # Add other relevant object columns\n",
    "\n",
    "for col in cols_to_encode:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            print(f\"Encoding categorical column: {col}\")\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            label_encoders[col] = le # Store encoder if needed later\n",
    "        else:\n",
    "             print(f\"Skipping encoding for non-object column: {col}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found for encoding.\")\n",
    "print(f\"DataFrame shape after encoding: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 6: Feature Selection ---\")\n",
    "\n",
    "# Drop columns that don't add value for ML predictions\n",
    "columns_to_drop = [\n",
    "    'Url_spotify',\n",
    "    'Uri',\n",
    "    'Url_youtube',\n",
    "    'Description',\n",
    "    'Licensed',\n",
    "    'official_video'\n",
    "]\n",
    "\n",
    "# Drop the specified columns\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped columns: {columns_to_drop}\")\n",
    "print(f\"DataFrame shape after feature selection: {df.shape}\")\n",
    "\n",
    "# Display remaining columns\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 6.5: Feature Correlation Analysis ---\")\n",
    "\n",
    "# Create a correlation matrix for numerical features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select only numeric columns for correlation analysis\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "print(\"Correlation heatmap saved as 'correlation_heatmap.png'\")\n",
    "\n",
    "\n",
    "# Create a Confusion Matrix\n",
    "plt. \n",
    "# Display top correlations with the target variable\n",
    "if TARGET_COLUMN in numeric_df.columns:\n",
    "    target_correlations = corr_matrix[TARGET_COLUMN].sort_values(ascending=False)\n",
    "    print(f\"\\nTop correlations with {TARGET_COLUMN}:\")\n",
    "    print(target_correlations)\n",
    "else:\n",
    "    print(f\"\\nTarget column {TARGET_COLUMN} not found in numeric columns\")\n",
    "\n",
    "# Check for multicollinearity (features with high correlation > 0.8)\n",
    "print(\"\\nChecking for multicollinearity (corr > 0.8):\")\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"Features with high correlation (potential multicollinearity):\")\n",
    "    for feat1, feat2, corr in high_corr:\n",
    "        print(f\"  {feat1} & {feat2}: {corr:.2f}\")\n",
    "else:\n",
    "    print(\"No high correlation detected between features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 6.6: PCA Analysis for View Prediction ---\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We need to work with only numerical features for PCA (excluding our target variable Views)\n",
    "numeric_features = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Make sure to remove the target variable (Views) from the features for PCA\n",
    "if TARGET_COLUMN in numeric_features.columns:\n",
    "    target_views = numeric_features[TARGET_COLUMN].copy()\n",
    "    numeric_features = numeric_features.drop(columns=[TARGET_COLUMN])\n",
    "    print(f\"Target variable '{TARGET_COLUMN}' separated for visualization with PCA features\")\n",
    "else:\n",
    "    print(f\"Warning: Target variable '{TARGET_COLUMN}' not found in numeric features\")\n",
    "    # If we don't have Views in our dataframe at this point, something went wrong\n",
    "    target_views = None\n",
    "\n",
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(numeric_features)\n",
    "print(f\"Features standardized, shape: {scaled_features.shape}\")\n",
    "\n",
    "# Apply PCA with 2 components for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_features)\n",
    "print(f\"PCA applied, reduced to {pca_result.shape[1]} dimensions\")\n",
    "\n",
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Add the target variable for coloring (only if we have it)\n",
    "if target_views is not None:\n",
    "    # For better visualization with a skewed target, we can log-transform Views\n",
    "    log_views = np.log10(target_views + 1)  # Add 1 to avoid log(0)\n",
    "    pca_df['Log_Views'] = log_views\n",
    "    print(f\"Added log-transformed {TARGET_COLUMN} for visualization\")\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained variance ratio: PC1 = {explained_variance[0]:.4f}, PC2 = {explained_variance[1]:.4f}\")\n",
    "print(f\"Total variance explained: {sum(explained_variance):.4f}\")\n",
    "\n",
    "# Create a scatter plot of PCA results (2D)\n",
    "plt.figure(figsize=(10, 8))\n",
    "if target_views is not None:\n",
    "    # Color by log-transformed Views\n",
    "    scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], \n",
    "                       c=pca_df['Log_Views'], \n",
    "                       cmap='viridis', \n",
    "                       alpha=0.6)\n",
    "    plt.colorbar(scatter, label=f'Log10({TARGET_COLUMN})')\n",
    "    plt.title(f'PCA: Feature Projection with {TARGET_COLUMN} (log-scale) as Color')\n",
    "else:\n",
    "    # Simple scatter plot without target coloring\n",
    "    plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5)\n",
    "    plt.title('PCA: 2-Component Projection (without target variable)')\n",
    "\n",
    "plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.1%} variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.1%} variance)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('pca_visualization_views.png')\n",
    "print(\"PCA visualization saved as 'pca_visualization_views.png'\")\n",
    "\n",
    "# 3D PCA Visualization with 3 components\n",
    "print(\"\\n--- Creating 3D PCA Visualization ---\")\n",
    "# Rerun PCA with 3 components\n",
    "pca_3d = PCA(n_components=3)\n",
    "pca_result_3d = pca_3d.fit_transform(scaled_features)\n",
    "explained_variance_3d = pca_3d.explained_variance_ratio_\n",
    "print(f\"PCA 3D applied, reduced to 3 dimensions\")\n",
    "print(f\"Explained variance ratio: PC1={explained_variance_3d[0]:.4f}, PC2={explained_variance_3d[1]:.4f}, PC3={explained_variance_3d[2]:.4f}\")\n",
    "print(f\"Total variance explained by 3 components: {sum(explained_variance_3d):.4f}\")\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "if target_views is not None:\n",
    "    # Create a color map based on log-transformed views\n",
    "    log_views = np.log10(target_views + 1)\n",
    "    scatter = ax.scatter(\n",
    "        pca_result_3d[:, 0],  # PC1\n",
    "        pca_result_3d[:, 1],  # PC2\n",
    "        pca_result_3d[:, 2],  # PC3\n",
    "        c=log_views,\n",
    "        cmap='viridis',\n",
    "        alpha=0.6,\n",
    "        s=30  # Point size\n",
    "    )\n",
    "    fig.colorbar(scatter, ax=ax, label=f'Log10({TARGET_COLUMN})')\n",
    "else:\n",
    "    ax.scatter(\n",
    "        pca_result_3d[:, 0],\n",
    "        pca_result_3d[:, 1],\n",
    "        pca_result_3d[:, 2],\n",
    "        alpha=0.6,\n",
    "        s=30\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({explained_variance_3d[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({explained_variance_3d[1]:.1%})')\n",
    "ax.set_zlabel(f'PC3 ({explained_variance_3d[2]:.1%})')\n",
    "ax.set_title('3D PCA Projection of Features')\n",
    "\n",
    "# Add a grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('pca_3d_visualization.png')\n",
    "print(\"3D PCA visualization saved as 'pca_3d_visualization.png'\")\n",
    "\n",
    "# Feature importance in PCA\n",
    "component_names = [f\"PC{i+1}\" for i in range(2)]\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T, \n",
    "    columns=component_names, \n",
    "    index=numeric_features.columns\n",
    ")\n",
    "\n",
    "# Display feature contributions to PCA components\n",
    "print(\"\\nFeature contributions to principal components:\")\n",
    "print(loadings)\n",
    "\n",
    "# Plot the feature loadings\n",
    "plt.figure(figsize=(12, 10))\n",
    "loadings_plot = sns.heatmap(loadings, cmap='coolwarm', annot=True, fmt=\".3f\")\n",
    "plt.title('PCA Feature Loadings for View Prediction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_loadings_views.png')\n",
    "print(\"PCA loadings heatmap saved as 'pca_loadings_views.png'\")\n",
    "\n",
    "# Add a visualization to show how PC1 and PC2 correlate with Views\n",
    "if target_views is not None:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # PC1 vs Views\n",
    "    ax1.scatter(pca_df['PC1'], target_views, alpha=0.5, c='royalblue')\n",
    "    ax1.set_xlabel('Principal Component 1')\n",
    "    ax1.set_ylabel(TARGET_COLUMN)\n",
    "    ax1.set_title(f'PC1 vs {TARGET_COLUMN}')\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # PC2 vs Views\n",
    "    ax2.scatter(pca_df['PC2'], target_views, alpha=0.5, c='forestgreen')\n",
    "    ax2.set_xlabel('Principal Component 2')\n",
    "    ax2.set_ylabel(TARGET_COLUMN)\n",
    "    ax2.set_title(f'PC2 vs {TARGET_COLUMN}')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_vs_views.png')\n",
    "    print(\"PCA components vs Views visualization saved as 'pca_vs_views.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 7: Splitting Data into Features (X) and Target (y) ---\")\n",
    "if TARGET_COLUMN not in df.columns:\n",
    "    print(f\"Error: Target column '{TARGET_COLUMN}' not found in DataFrame.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "X = df.drop(TARGET_COLUMN, axis=1)\n",
    "y = df[TARGET_COLUMN]\n",
    "print(f\"Features shape (X): {X.shape}\")\n",
    "print(f\"Target shape (y): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 8: Splitting Data into Training and Testing Sets ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"Data split into training and testing sets (test_size={TEST_SIZE}, random_state={RANDOM_STATE}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 9: Final Shapes ---\")\n",
    "print(\"Training features shape (X_train):\", X_train.shape)\n",
    "print(\"Testing features shape (X_test):\", X_test.shape)\n",
    "print(\"Training target shape (y_train):\", y_train.shape)\n",
    "print(\"Testing target shape (y_test):\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 10: Basic Linear Regression Model ---\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import math\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "print(\"Training a basic linear regression model...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = math.sqrt(train_mse)\n",
    "test_rmse = math.sqrt(test_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nLinear Regression Model Performance:\")\n",
    "print(f\"Training set MSE: {train_mse:.2f}\")\n",
    "print(f\"Test set MSE: {test_mse:.2f}\")\n",
    "print(f\"Training set RMSE: {train_rmse:.2f}\")\n",
    "print(f\"Test set RMSE: {test_rmse:.2f}\")\n",
    "print(f\"Training set MAE: {train_mae:.2f}\")\n",
    "print(f\"Test set MAE: {test_mae:.2f}\")\n",
    "print(f\"Training set R²: {train_r2:.4f}\")\n",
    "print(f\"Test set R²: {test_r2:.4f}\")\n",
    "\n",
    "# Check for feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "})\n",
    "feature_importance['Abs_Coefficient'] = abs(feature_importance['Coefficient'])\n",
    "feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 most important features in linear regression model:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot actual vs predicted values on test set\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Views')\n",
    "plt.ylabel('Predicted Views')\n",
    "plt.title('Linear Regression: Actual vs Predicted Views (Test Set)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('linear_regression_actual_vs_predicted.png')\n",
    "print(\"Linear regression actual vs predicted plot saved as 'linear_regression_actual_vs_predicted.png'\")\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 8))\n",
    "residuals = y_test - y_test_pred\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel('Predicted Views')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Linear Regression: Residuals Plot')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('linear_regression_residuals.png')\n",
    "print(\"Linear regression residuals plot saved as 'linear_regression_residuals.png'\")\n",
    "\n",
    "# Try a log transformation for better visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(np.log10(y_test + 1), np.log10(y_test_pred + 1), alpha=0.5)\n",
    "plt.plot([0, np.log10(y_test.max() + 1)], [0, np.log10(y_test.max() + 1)], 'r--')\n",
    "plt.xlabel('Log10(Actual Views)')\n",
    "plt.ylabel('Log10(Predicted Views)')\n",
    "plt.title('Linear Regression: Log-transformed Actual vs Predicted Views')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.savefig('linear_regression_log_transformed.png')\n",
    "print(\"Log-transformed actual vs predicted plot saved as 'linear_regression_log_transformed.png'\")\n",
    "\n",
    "print(\"\\n--- Basic Linear Regression Analysis Complete ---\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 11: KNN Classifier ---\")\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,accuracy_score, precision_score,recall_score,f1_score\n",
    "import math\n",
    "\n",
    "#Convert into 5 categories\n",
    "# 1: 0-1million\n",
    "# 2: 1-10 million\n",
    "# 3: 10-100 million\n",
    "# 4: 100-1billion\n",
    "# 5: 1 billion+\n",
    "knn_bins = [0, 1_000_000, 10_000_000, 100_000_000, 1_000_000_000, float('inf')]\n",
    "knn_labels = [1,2,3,4,5]\n",
    "\n",
    "#Adjust training and testing values\n",
    "knn_y_train = pd.cut(y_train, bins=knn_bins, labels=knn_labels, right=False)\n",
    "knn_y_test = pd.cut(y_test, bins=knn_bins, labels=knn_labels, right = False)\n",
    "\n",
    "#Initalize and train the KNN Classifier Model\n",
    "print(\"Training a knn classifer model...\")\n",
    "knn_model = StackingClassifier(estimators=[(\"dt\", DecisionTreeClassifier()), (\"knn\", KNeighborsClassifier())], final_estimator=LogisticRegression())\n",
    "knn_model.fit(X_train, knn_y_train)\n",
    "\n",
    "#Knn Predictions on training and test data\n",
    "knn_train_pred = knn_model.predict(X_train)\n",
    "knn_test_pred = knn_model.predict(X_test)\n",
    "\n",
    "#Evaluate model \n",
    "knn_train_confusion = confusion_matrix(knn_y_train, knn_train_pred)\n",
    "knn_test_confusion = confusion_matrix(knn_y_test, knn_test_pred)\n",
    "knn_train_cm =ConfusionMatrixDisplay(confusion_matrix = knn_train_confusion, display_labels=[1,2,3,4,5])\n",
    "knn_test_cm =ConfusionMatrixDisplay(confusion_matrix = knn_test_confusion, display_labels=[1,2,3,4,5])\n",
    "knn_train_accuracy = accuracy_score(knn_y_train, knn_train_pred)\n",
    "knn_test_accuracy = accuracy_score(knn_y_test, knn_test_pred)\n",
    "knn_train_precision = precision_score(knn_y_train, knn_train_pred, average = 'weighted')\n",
    "knn_test_precision = precision_score(knn_y_test, knn_test_pred, average = 'weighted')\n",
    "knn_train_recall = recall_score(knn_y_train, knn_train_pred, average = 'weighted')\n",
    "knn_test_recall = recall_score(knn_y_test, knn_test_pred, average = 'weighted')\n",
    "knn_train_f1 = f1_score(knn_y_train, knn_train_pred, average = 'weighted')\n",
    "knn_test_f1 = f1_score(knn_y_test, knn_test_pred, average = 'weighted')\n",
    "\n",
    "#Print Evalutations \n",
    "print(\"\\nKNN Classification Model Performance:\")\n",
    "print(\"\")\n",
    "knn_train_cm.plot()\n",
    "plt.title(\"KNN Classification on Training Set\")\n",
    "plt.show()\n",
    "knn_test_cm.plot()\n",
    "plt.title(\"KNN Classification on Testing Set\")\n",
    "plt.show()\n",
    "print(f\"Training set accuracy: {knn_train_accuracy:.4f}\")\n",
    "print(f\"Testing set accuracy: {knn_test_accuracy:.4f}\")\n",
    "print(f\"Training set precision: {knn_train_precision:.4f}\")\n",
    "print(f\"Testing set precision: {knn_test_precision:.4f}\")\n",
    "print(f\"Training set recall: {knn_train_recall:.4f}\")\n",
    "print(f\"Testing set recall: {knn_test_recall:.4f}\")\n",
    "print(f\"Training set F1-Score: {knn_train_f1:.4f}\")\n",
    "print(f\"Testing set F1-Score: {knn_test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11_DBMS",
   "language": "python",
   "name": "py11_dbms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
